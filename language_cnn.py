import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import re
import pickle  # ç”¨æ¥ä¿å­˜è¯è¡¨
import os
import pandas as pd

# ==========================================
# 0. GPU è®¾å¤‡é…ç½® (æ ¸å¿ƒæ­¥éª¤)
# ==========================================
# æ£€æŸ¥æ˜¯å¦æœ‰ NVIDIA æ˜¾å¡ï¼Œæ²¡æœ‰åˆ™ä½¿ç”¨ CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ å½“å‰ä½¿ç”¨çš„è®¡ç®—è®¾å¤‡: {device}")
if device.type == 'cuda':
    print(f"   æ˜¾å¡åç§°: {torch.cuda.get_device_name(0)}")

# ==========================================
# 1. é…ç½®ä¸æ¨¡å‹å®šä¹‰ (ä¿æŒä¸å˜)
# ==========================================
class Config:
    vocab_size = 5000
    embed_dim = 100
    filter_sizes = [3, 4, 5]
    num_filters = 100
    num_classes = 2
    dropout = 0.5
    batch_size = 2
    lr = 0.001
    epochs = 20
    max_len = 20
    model_save_path = "textcnn_model.pth"   # æ¨¡å‹æƒé‡ä¿å­˜è·¯å¾„
    vocab_save_path = "vocab.pkl"           # è¯è¡¨ä¿å­˜è·¯å¾„

# æ•°æ®é¢„å¤„ç†å·¥å…·
def tokenizer(text):
    text = re.sub(r'[^\w\s]', '', text).lower()
    return text.split()

class TextCNN(nn.Module):
    def __init__(self, vocab_size, embed_dim, filter_sizes, num_filters, num_classes, dropout):
        super(TextCNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.convs = nn.ModuleList([
            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=fs) 
            for fs in filter_sizes
        ])
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        x = x.permute(0, 2, 1)
        conved = [F.relu(conv(x)) for conv in self.convs]
        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]
        cat = torch.cat(pooled, dim=1)
        cat = self.dropout(cat)
        logits = self.fc(cat)
        return logits

# Dataset å®šä¹‰
class TextDataset(Dataset):
    def __init__(self, data, word2idx, max_len):
        self.data = data
        self.word2idx = word2idx
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        text, label = self.data[index]
        tokens = tokenizer(text)
        token_ids = [self.word2idx.get(token, self.word2idx["<UNK>"]) for token in tokens]
        if len(token_ids) < self.max_len:
            token_ids += [0] * (self.max_len - len(token_ids))
        else:
            token_ids = token_ids[:self.max_len]
        return torch.tensor(token_ids), torch.tensor(label)

# ==========================================
# 2. è®­ç»ƒæµç¨‹ (åŒ…å« GPU æ“ä½œ)
# ==========================================
def train():
    # æ¨¡æ‹Ÿæ•°æ®
    df=pd.read_csv("IMDBDataset.csv")
    df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})
    raw_data = list(zip(df['review'][:1000], df['sentiment'][:1000]))

    # æ„å»ºè¯è¡¨
    word2idx = {"<PAD>": 0, "<UNK>": 1}
    idx = 2
    for text, label in raw_data:
        for word in tokenizer(text):
            if word not in word2idx:
                word2idx[word] = idx
                idx += 1
    
    # å‡†å¤‡ DataLoader
    dataset = TextDataset(raw_data, word2idx, Config.max_len)
    dataloader = DataLoader(dataset, batch_size=Config.batch_size, shuffle=True)

    # åˆå§‹åŒ–æ¨¡å‹ï¼Œå¹¶æ¬è¿åˆ° GPU !!!
    model = TextCNN(len(word2idx), Config.embed_dim, Config.filter_sizes, 
                    Config.num_filters, Config.num_classes, Config.dropout)
    model = model.to(device)  # <--- å…³é”®æ­¥éª¤ï¼šæ¨¡å‹æ¬å®¶

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=Config.lr)

    print("\n--- å¼€å§‹è®­ç»ƒ ---")
    model.train()
    for epoch in range(Config.epochs):
        total_loss = 0
        for batch_x, batch_y in dataloader:
            # æ•°æ®æ¬è¿åˆ° GPU !!!
            batch_x = batch_x.to(device)  # <--- å…³é”®æ­¥éª¤ï¼šè¾“å…¥æ¬å®¶
            batch_y = batch_y.to(device)  # <--- å…³é”®æ­¥éª¤ï¼šæ ‡ç­¾æ¬å®¶
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if (epoch+1) % 5 == 0:
            print(f"Epoch [{epoch+1}/{Config.epochs}], Loss: {total_loss/len(dataloader):.4f}")

    # ==========================================
    # 3. ä¿å­˜æ¨¡å‹ä¸è¯è¡¨
    # ==========================================
    print("\n--- ä¿å­˜æ¨¡å‹ ---")
    # 1. ä¿å­˜æ¨¡å‹å‚æ•° (state_dict)
    torch.save(model.state_dict(), Config.model_save_path)
    print(f"âœ… æ¨¡å‹å‚æ•°å·²ä¿å­˜è‡³: {Config.model_save_path}")

    # 2. ä¿å­˜è¯è¡¨ (word2idx)
    # è¿™æ­¥è‡³å…³é‡è¦ï¼Œæ²¡æœ‰è¯è¡¨ï¼Œæ¨¡å‹å°±æ˜¯åºŸé“
    with open(Config.vocab_save_path, 'wb') as f:
        pickle.dump(word2idx, f)
    print(f"âœ… è¯è¡¨å·²ä¿å­˜è‡³: {Config.vocab_save_path}")

# ==========================================
# 4. åŠ è½½ä¸æ¨ç† (æ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒè°ƒç”¨)
# ==========================================
class SentimentPredictor:
    def __init__(self):
        # 1. åŠ è½½è¯è¡¨
        if not os.path.exists(Config.vocab_save_path):
            raise FileNotFoundError("æ‰¾ä¸åˆ°è¯è¡¨æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒï¼")
            
        with open(Config.vocab_save_path, 'rb') as f:
            self.word2idx = pickle.load(f)
        
        # 2. åˆå§‹åŒ–æ¨¡å‹ç»“æ„ (å‚æ•°å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´)
        self.model = TextCNN(len(self.word2idx), Config.embed_dim, Config.filter_sizes, 
                             Config.num_filters, Config.num_classes, Config.dropout)
        
        # 3. åŠ è½½æƒé‡
        # map_locationç¡®ä¿åœ¨æ²¡æœ‰GPUçš„æœºå™¨ä¸Šä¹Ÿèƒ½åŠ è½½GPUè®­ç»ƒçš„æ¨¡å‹
        self.model.load_state_dict(torch.load(Config.model_save_path, map_location=device))
        
        # 4. æ¬è¿åˆ° GPU
        self.model = self.model.to(device)
        self.model.eval() # å¼€å¯è¯„ä¼°æ¨¡å¼ (å…³é—­Dropout)
        
        print("ğŸ‰ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œéšæ—¶å¾…å‘½ï¼")

    def predict(self, text):
        # æ•°æ®é¢„å¤„ç†
        tokens = tokenizer(text)
        token_ids = [self.word2idx.get(t, self.word2idx["<UNK>"]) for t in tokens]
        
        # Padding
        if len(token_ids) < Config.max_len:
            token_ids += [0] * (Config.max_len - len(token_ids))
        else:
            token_ids = token_ids[:Config.max_len]
        
        # è½¬ Tensor å¹¶æ¬è¿åˆ° GPU
        tensor_input = torch.tensor(token_ids).unsqueeze(0).to(device)
        
        with torch.no_grad():
            outputs = self.model(tensor_input)
            probs = F.softmax(outputs, dim=1)
            pred_idx = torch.argmax(probs, dim=1).item()
            
        return pred_idx, probs[0][pred_idx].item()

# ==========================================
# ä¸»ç¨‹åºå…¥å£
# ==========================================
if __name__ == '__main__':
    # ç¬¬ä¸€æ¬¡è¿è¡Œï¼šè®­ç»ƒå¹¶ä¿å­˜
    train()
    
    # æ¨¡æ‹Ÿï¼šé‡å¯ç¨‹åºåï¼Œç›´æ¥åŠ è½½æ¨¡å‹è¿›è¡Œé¢„æµ‹
    print("\n--- æ¨¡æ‹Ÿé‡æ–°åŠ è½½æ¨¡å‹ ---")
    predictor = SentimentPredictor()
    
    # æµ‹è¯•
    test_sentences = [
        "This is the best movie I have seen",
        "Absolutely garbage, do not watch"
    ]
    
    for sent in test_sentences:
        label, conf = predictor.predict(sent)
        res_str = "ç§¯æ ğŸ˜Š" if label == 1 else "æ¶ˆæ ğŸ˜¡"
        print(f"è¯­å¥: {sent}\né¢„æµ‹: {res_str} (ç½®ä¿¡åº¦: {conf*100:.2f}%)")
        print("-" * 30)